{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AntiVirus Demo\n",
    "\n",
    "I will test 6 different machine learning classification algorithms to detecting before selecting the one with the highest probability of successful detections. \n",
    "\n",
    "The classifiers I will be testing will be:\n",
    "\n",
    "+ Decision Tree.\n",
    "+ Random Forest.\n",
    "+ Gradient Boosting. \n",
    "+ Adaboost.\n",
    "+ Extra Trees Classifier\n",
    "+ GNB.\n",
    "\n",
    "The algorithms will be training on a csv file dataset containing 138045 PE files. \n",
    "\n",
    "I will then test a variety of PE files to see whether the winning algorithm is successful with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Machine learning only uses integers and floats as features for detection but most PE file parameters are integers which will be enough for our detection model. I used a compiled CSV file containing a list of extracted features from llSourcell  \n",
    "\n",
    "\n",
    "The list of feature extracted : \n",
    "\n",
    "Name, md5, Machine, SizeOfOptionalHeader, Characteristics, MajorLinkerVersion, MinorLinkerVersion, SizeOfCode, SizeOfInitializedData, SizeOfUninitializedData, AddressOfEntryPoint, BaseOfCode, BaseOfData, ImageBase, SectionAlignment, FileAlignment, MajorOperatingSystemVersion, MinorOperatingSystemVersion, MajorImageVersion, MinorImageVersion, MajorSubsystemVersion, MinorSubsystemVersion, SizeOfImage, SizeOfHeaders, CheckSum, Subsystem, DllCharacteristics, SizeOfStackReserve, SizeOfStackCommit, SizeOfHeapReserve, SizeOfHeapCommit, LoaderFlags, NumberOfRvaAndSizes, SectionsNb, SectionsMeanEntropy, SectionsMinEntropy, SectionsMaxEntropy, SectionsMeanRawsize, SectionsMinRawsize, SectionMaxRawsize, SectionsMeanVirtualsize, SectionsMinVirtualsize, SectionMaxVirtualsize, ImportsNbDLL, ImportsNb, ImportsNbOrdinal, ExportNb, ResourcesNb, ResourcesMeanEntropy, ResourcesMinEntropy, ResourcesMaxEntropy, ResourcesMeanSize, ResourcesMinSize, ResourcesMaxSize, LoadConfigurationSize, VersionInformationSize.\n",
    "\n",
    "\n",
    "Regarding the dataset, we need to have an important number of both legitimate and malicious file:\n",
    "\n",
    "For legitimate file, the dataset contains all the Windows binaries (exe + dll) from Windows 2008, Windows XP and Windows 7 32 and 64 bits, so exactly 41323 binaries. It is not a perfect dataset as there is only Microsoft binaries and not binaries from application which could have different properties, but  it will be enough for testing. \n",
    "    \n",
    "Regarding malware, part of Virus Share collection was used by downloading one archive (the 134th) and kept only PE files (96724 different files).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "For feature selection we need to reduce the number of features to a smaller set of the most relevant, differentiating malicious binaries from legitimate. To do this we will use Pandas, Sklearn and Numpy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn import cross_validation, tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\"\"\"Import dataset and drop irrelivant x columns. Data set contains a list of of portable \n",
    "executable files labeled as legit(1) or malicious (0)\"\"\"\n",
    "\n",
    "data = pd.read_csv('data.csv', sep='|')\n",
    "legit_binaries = data[0:41323].drop(['legitimate'], axis=1)\n",
    "malicious_binaries = data[41323::].drop(['legitimate'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check manually for differences we can check the different values and see if there is a difference between the two by taking the FileAlignment parameter and check its values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512      36843\n",
       "4096      4313\n",
       "128         89\n",
       "32          40\n",
       "65536       36\n",
       "16           2\n",
       "Name: FileAlignment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_binaries['FileAlignment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512     94612\n",
       "4096     2074\n",
       "128        18\n",
       "1024       15\n",
       "64          2\n",
       "32          1\n",
       "16          1\n",
       "2048        1\n",
       "Name: FileAlignment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_binaries['FileAlignment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much difference in the values and wouldn't make a good feature to select from. However other features such as the max entropy of sections are. We can show this by visualising a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n",
      "C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([legit_binaries['SectionsMaxEntropy'], malicious_binaries['SectionsMaxEntropy']], range=[0,8],\n",
    "         normed=True, color=[\"green\", \"red\"],label=[\"legitimate\", \"malicious\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the malicious files have higher entropy than those of legitimate which is a good start for feature selection.\n",
    "\n",
    "However, the Scikit library has a more efficient way of selecting fetaures to reduce dimentionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 features identified as important:\n",
      "1. feature Characteristics (0.173984)\n",
      "2. feature Machine (0.084511)\n",
      "3. feature ImageBase (0.080936)\n",
      "4. feature DllCharacteristics (0.065589)\n",
      "5. feature ResourcesMaxEntropy (0.063370)\n",
      "6. feature VersionInformationSize (0.058164)\n",
      "7. feature MajorSubsystemVersion (0.057989)\n",
      "8. feature ResourcesMinEntropy (0.056979)\n",
      "9. feature SectionsMaxEntropy (0.049482)\n",
      "10. feature SizeOfOptionalHeader (0.047643)\n",
      "11. feature Subsystem (0.043562)\n",
      "12. feature MajorOperatingSystemVersion (0.030425)\n",
      "13. feature SectionsMinEntropy (0.021421)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Feature selection using Trees Classifier: we need to check which of the data is important \n",
    "for the classifier. Using an extra trees classifier.It fits randomised decision trees to subsets of our data\"\"\" \n",
    "\n",
    "fsel = ske.ExtraTreesClassifier().fit(X, y)\n",
    "model = SelectFromModel(fsel, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "nb_features = X_new.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_new, y ,test_size=0.2)\n",
    "\n",
    "features = []\n",
    "\n",
    "print('%i features identified as important:' % nb_features)\n",
    "\n",
    "#sort features \n",
    "indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]\n",
    "for f in range(nb_features):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))\n",
    "\n",
    "# XXX : take care of the feature order\n",
    "for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):\n",
    "    features.append(data.columns[2+f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case 13 features were regarded as important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Selection\n",
    "\n",
    "So to decide model to use a simple option is test a variety using their default parameters and use the one wone with the best success rate. This is judged by the lowest false positive and false negative rate. As mentioned earlier, we will be testing: Decision Tree,\n",
    "Random Forest, \n",
    "Gradient Boosting, Extra Trees Classifier,  \n",
    "Adaboost and\n",
    "GNB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now testing algorithms\n",
      "DecisionTree : 99.090909 %\n",
      "RandomForest : 99.424122 %\n",
      "ExtraTreesClassifier : 99.413256 %\n",
      "GradientBoosting : 98.797537 %\n",
      "AdaBoost : 98.649040 %\n",
      "GNB : 70.079681 %\n",
      "\n",
      "Winner algorithm is RandomForest with a 99.424122 % success\n",
      "False positive rate : 0.475477 %\n",
      "False negative rate : 0.811040 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We need to create an array of models: need to test each algorithm \n",
    "Algorithm comparison\"\"\"\n",
    "\n",
    "algorithms = {\n",
    "        \"DecisionTree\": tree.DecisionTreeClassifier(max_depth=10),\n",
    "        \"RandomForest\": ske.RandomForestClassifier(n_estimators=50),\n",
    "        \"ExtraTreesClassifier\": ske.ExtraTreesClassifier(n_estimators=50),\n",
    "        \"GradientBoosting\": ske.GradientBoostingClassifier(n_estimators=50),\n",
    "        \"AdaBoost\": ske.AdaBoostClassifier(n_estimators=100),\n",
    "        \"GNB\": GaussianNB()\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "print(\"\\nNow testing algorithms\")\n",
    "for algo in algorithms:\n",
    "    clf = algorithms[algo]\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"%s : %f %%\" % (algo, score*100))\n",
    "    results[algo] = score\n",
    "\n",
    "winner = max(results, key=results.get)\n",
    "print('\\nWinner algorithm is %s with a %f %% success' % (winner, results[winner]*100))\n",
    "\n",
    "\n",
    "\n",
    "# Identify false and true positive rates\n",
    "clf = algorithms[winner]\n",
    "res = clf.predict(X_test)\n",
    "mt = confusion_matrix(y_test, res)\n",
    "print(\"False positive rate : %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
    "print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see the RandomForest algorithm worked the best with fp/fn score of: \n",
    "\n",
    "+ False positive rate : 0.475477 %\n",
    "+ False negative rate : 0.811040 %\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "<img src=\"random forest.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "Random decision forests correct for decision trees' habit of overfitting to their training set so the high success rate maybe due to overfitting. Overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\". One way to test this is by testing it on newer malicious PE files another solution would be to retrain the model periodically on new data but this somewhat defeats the point of AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification Selection\n",
    "\n",
    "So to test the algoritms on new data we need to save the object and feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving algorithm and feature list in classifier directory...\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Save the algorithm and the feature list for later predictions\n",
    "print('Saving algorithm and feature list in classifier directory...')\n",
    "joblib.dump(algorithms[winner], 'classifier/classifier.pkl')\n",
    "open('classifier/features.pkl', 'wb').write(pickle.dumps(features))\n",
    "print('Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to implement a function to load the feature list, extract the PE featues and then predict whether the file is legitimate or not. To do this I used a checkpe.py function to do so which you can find in the file folder. \n",
    "\n",
    "To run the function: open a command line and enter:\n",
    "\n",
    "```python checkpe.py nameoffile.pe```\n",
    "\n",
    "Here is a snapshot from when I tested it on some legitimate and malicious files. \n",
    "\n",
    "<img src=\"test.png\" width=\"1000\">\n",
    "\n",
    "As you can see it worked well enough on these simple .dll, .sys, .exe files. However, I have to stress this model will only work on PE features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Whilst the accuracy is better than some products out there it still falls short and this has to do with the differences between specitivity, specificity, recall and precision: \n",
    "\n",
    "\n",
    "    Sensitivity: Of all labeled positive for containing malware, how many were correctly predicted?\n",
    "    Specificity: Of all labeled negative for containing malware, how many were correctly predicted?\n",
    "    \n",
    "    Recall: Of all the files containing malware, how many did we predict as containing?\n",
    "    Precision: Of all the files containing malware, how many actually had malware?\n",
    "    \n",
    "    False Positive: the proportion of events badly identified as positive over the total number of negatives\n",
    "    False Negative: the proportion of events badly identified as negative over the total number of positives\n",
    "    \n",
    "\n",
    "<img src=\"confusion_matrix_1.png\" width=\"400\">\n",
    "\n",
    "So the problem is that although 99.4% accuracy is high you cannot just consider the malicious over legitimate ratio per the amount of alerts the programm generates. If we look at the confusion matrix below we can see that out of 27610 identified features 92 came out as false negative which is a substantial amount to get through. \n",
    "\n",
    "It may be possible to make a neural net that could predict more efficiently through backpropogation but that would require 10's of 1000's of files to test on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19257    92]\n",
      " [   67  8194]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mt = confusion_matrix(y_test, res)\n",
    "\n",
    "print (mt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
